{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative experiments with Credit Card Approval dataset\n",
    "This dataset contains information about bank customers, as well as information regarding theirÂ debt payments (if any). https://cutt.ly/xQ1mqyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New')\n",
    "model_path = 'C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New\\\\experiments\\\\quantitative\\\\Models\\\\D2\\\\'\n",
    "weights_path = 'C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New\\\\experiments\\\\quantitative\\\\Weights\\\\D2\\\\'\n",
    "data_path = 'C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New\\\\experiments\\\\quantitative\\\\Preprocessed Data\\\\D2\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "import warnings\n",
    "import json\n",
    "import lime.lime_tabular as lt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, concatenate\n",
    "import json\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from innvestigate.utils.keras import checks\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from altruist import Altruist\n",
    "from meta_explain import MetaExplain\n",
    "from utilities.dataset import Dataset\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "import numpy as np\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load the Credit Approval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in final dataset: (35442, 13)\n",
      "Original dataset shape Counter({0: 34846, 1: 596})\n",
      "TomekLinks: Resampled dataset shape Counter({0: 34649, 1: 596})\n",
      "NC: Resampled dataset shape Counter({0: 33196, 1: 596})\n",
      "NM: Resampled dataset shape Counter({0: 596, 1: 596})\n",
      "Random: Resampled dataset shape Counter({0: 596, 1: 596})\n"
     ]
    }
   ],
   "source": [
    "credit = Dataset()\n",
    "X_res, X, y_res, y, feature_names = credit.load_credit_approval(\n",
    "    original_data_available=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, stratify=y_res, random_state=42)\n",
    "class_names = ['Denial', 'Approval']\n",
    "feature_names = list(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will build two neural network models. One linear and one non-linear, and one uneccessary complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "transformer = MaxAbsScaler().fit(X_train)\n",
    "\n",
    "X_train = transformer.transform(X_train)\n",
    "X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\iamollas\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\iamollas\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "linear_neural = Sequential()\n",
    "linear_neural.add(Dense(1, activation='sigmoid',\n",
    "                        input_shape=(len(X_train[0]),)))\n",
    "linear_neural.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "neural = Sequential()\n",
    "neural.add(Dense(160, activation='relu', input_shape=(len(X_train[0]),)))\n",
    "neural.add(Dense(80, activation='tanh'))\n",
    "neural.add(Dense(40, activation='tanh'))\n",
    "neural.add(Dense(20, activation='tanh'))\n",
    "neural.add(Dense(16, activation='tanh'))\n",
    "neural.add(Dense(1, activation='sigmoid'))\n",
    "neural.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "complex_input = Input(shape=(len(X_train[0]),))\n",
    "complex_r = []\n",
    "for i in range(12):\n",
    "    temp_layer = Dense(units=128, activation='relu')(complex_input)\n",
    "    temp_layer = Dropout(0.3)(temp_layer)\n",
    "    temp_layer = Dense(units=64, activation='selu')(temp_layer)\n",
    "    temp_layer = Dropout(0.3)(temp_layer)\n",
    "    temp_layer = Dense(units=32, activation='selu')(temp_layer)\n",
    "    complex_r.append(temp_layer)\n",
    "complex_r.append(complex_input)\n",
    "complex = concatenate(complex_r)\n",
    "complex = Dropout(0.3)(complex)\n",
    "complex = Dense(160, activation='tanh')(complex)\n",
    "complex = concatenate([complex, complex_input])\n",
    "complex = Dropout(0.3)(complex)\n",
    "complex = Dense(100, activation='tanh')(complex)\n",
    "complex = Dropout(0.3)(complex)\n",
    "complex_output = Dense(1, activation='sigmoid')(complex)\n",
    "complex_neural = Model(complex_input, complex_output)\n",
    "complex_neural.compile(optimizer=\"adam\", loss=['binary_crossentropy'])\n",
    "\n",
    "models = {'lNN': linear_neural, 'NN': neural, 'cNN': complex_neural}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(name, y_test, y_pred):\n",
    "    if type(y_pred[0]) == type(np.ndarray([1])):\n",
    "        y_pred = np.array([1 if i[0] > 0.5 else 0 for i in y_pred])\n",
    "    print(name)\n",
    "    print('\\t', 'F1:', f1_score(y_test, y_pred, average='macro'))\n",
    "    print('\\t', 'Precision:', precision_score(y_test, y_pred, average='macro'))\n",
    "    print('\\t', 'Recall:', recall_score(y_test, y_pred, average='macro'))\n",
    "    print('\\t', 'Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\iamollas\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "lNN\n",
      "\t F1: 0.5983193277310924\n",
      "\t Precision: 0.5983193277310924\n",
      "\t Recall: 0.5983193277310924\n",
      "\t Accuracy: 0.5983263598326359\n",
      "NN\n",
      "\t F1: 0.6903277769995797\n",
      "\t Precision: 0.6904161412358134\n",
      "\t Recall: 0.6903361344537815\n",
      "\t Accuracy: 0.6903765690376569\n",
      "cNN\n",
      "\t F1: 0.6984861227922625\n",
      "\t Precision: 0.699220396123051\n",
      "\t Recall: 0.6986344537815126\n",
      "\t Accuracy: 0.698744769874477\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "for name, model in models.items():\n",
    "    if train:\n",
    "        check_point = ModelCheckpoint(\n",
    "            \"D2_\"+name+\".hdf5\", monitor=\"val_loss\", verbose=0, save_best_only=True, mode=\"auto\")\n",
    "        model.fit(X_train, y_train, epochs=500, batch_size=32,\n",
    "                  validation_split=0.2, verbose=0, callbacks=[check_point])\n",
    "        model.load_weights(\"D2_\"+name+\".hdf5\")\n",
    "    else:\n",
    "        model.load_weights(model_path+\"D2_\"+name+\".hdf5\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    compute_scores(name, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will prepare our predict functions to work well with our python scripts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cNN(x):\n",
    "    prediction = models['cNN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "def predict_NN(x):\n",
    "    prediction = models['NN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "def predict_lNN(x):\n",
    "    prediction = models['lNN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "predict_functions = {'lNN': predict_lNN, 'NN': predict_NN, 'cNN': predict_cNN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our explainers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_generators(model):\n",
    "    Xs = iutils.to_list(model.outputs)\n",
    "    ret = []\n",
    "    for x in Xs:\n",
    "        layer, node_index, tensor_index = x._keras_history\n",
    "        if checks.contains_activation(layer, activation=\"sigmoid\"):\n",
    "            if isinstance(layer, keras.layers.Activation):\n",
    "                ret.append(layer.get_input_at(node_index))\n",
    "            else:\n",
    "                layer_wo_act = innvestigate.utils.keras.graph.copy_layer_wo_activation(\n",
    "                    layer)\n",
    "                ret.append(layer_wo_act(layer.get_input_at(node_index)))\n",
    "    modified_model = Model(input=model.input, output=ret)\n",
    "    modified_model.trainable = False\n",
    "    modified_model.compile(optimizer=\"adam\", loss=[\n",
    "                           'binary_crossentropy'], metrics=['accuracy'])\n",
    "    analyzer_IG = innvestigate.create_analyzer(\n",
    "        'integrated_gradients', modified_model, reference_inputs=16*[0])\n",
    "    analyzer_LRP = innvestigate.create_analyzer('lrp.z', modified_model)\n",
    "    return [analyzer_IG, analyzer_LRP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_NN = analyzer_generators(neural)\n",
    "analyzer_lNN = analyzer_generators(linear_neural)\n",
    "analyzer_cNN = analyzer_generators(complex_neural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare each interpretation technique to have the same format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_lNN(instance, predict_function):\n",
    "    return [i[0] for i in models['lNN'].get_weights()[0]]\n",
    "\n",
    "def fi_IG_NN(instance, predict_function):\n",
    "    return analyzer_NN[0].analyze(np.array([instance]))[0]\n",
    "\n",
    "def fi_LRP_NN(instance, predict_function):\n",
    "    return analyzer_NN[1].analyze(np.array([instance]))[0]\n",
    "\n",
    "def fi_IG_lNN(instance, predict_function):\n",
    "    return analyzer_lNN[0].analyze(np.array([instance]))[0]\n",
    "\n",
    "def fi_LRP_lNN(instance, predict_function):\n",
    "    return analyzer_lNN[1].analyze(np.array([instance]))[0]\n",
    "\n",
    "def fi_IG_cNN(instance, predict_function):\n",
    "    return analyzer_cNN[0].analyze(np.array([instance]))[0]\n",
    "\n",
    "def fi_LRP_cNN(instance, predict_function):\n",
    "    return analyzer_cNN[1].analyze(np.array([instance]))[0]\n",
    "\n",
    "explainer = lt.LimeTabularExplainer(training_data=X_train,\n",
    "                                    feature_names=feature_names, class_names=class_names,\n",
    "                                    discretize_continuous=True, mode='regression')\n",
    "\n",
    "def fi_lime(instance, predict_function):\n",
    "    def predict(x):\n",
    "        return np.array([i for i in predict_function(x)])\n",
    "    b = explainer.explain_instance(instance, predict,\n",
    "                                   num_features=len(list(feature_names)))[0].local_exp\n",
    "    b = b[list(b.keys())[0]]\n",
    "    b.sort()\n",
    "    return [i[1] for i in list(b)]\n",
    "\n",
    "def fi_random(instance, predict_function):\n",
    "    seed = (instance[0].sum() +\n",
    "            instance[0].mean() +\n",
    "            X_train[0][0] +\n",
    "            X_train[0][1])/10\n",
    "    random.seed(seed)\n",
    "    return [random.randrange(-1000, 1000)/1000 for i in range(len(feature_names))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute and save the interpretations for easier reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "\n",
    "    importance_train = []\n",
    "    for instance in X_train:\n",
    "        importance_instance = []\n",
    "        for fi in fi_techniques:\n",
    "            importance_instance.append(maxabs_scale(fi(instance, neural_type)))\n",
    "        importance_train.append(importance_instance)\n",
    "    importance_train = np.array(importance_train)\n",
    "\n",
    "    importance_test = []\n",
    "    for instance in X_test:\n",
    "        importance_instance = []\n",
    "        for fi in fi_techniques:\n",
    "            importance_instance.append(maxabs_scale(fi(instance, neural_type)))\n",
    "        importance_test.append(importance_instance)\n",
    "    importance_test = np.array(importance_test)\n",
    "\n",
    "    importances = {'train': importance_train.tolist(),\n",
    "                   'test': importance_test.tolist()}\n",
    "    with open('D2_'+neural_name+'.txt', 'w') as outfile:\n",
    "        json.dump(importances, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our quantitative experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D2_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_test = np.array(importances['test'])\n",
    "    meta_explain = MetaExplain(importance_train, feature_names)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['weak', 'normal', 'strong']:\n",
    "        for delta in [0 , 0.0001, 0.001, 0.01, 0.1]:\n",
    "            my_altruist = Altruist(\n",
    "                neural_type, X_train, fi_techniques, feature_names, level=noise, delta=delta)\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(X_test)):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(X_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                temp_meta = []\n",
    "                temp_meta.append(meta_explain.meta_avg(b))\n",
    "                temp_meta.append(meta_explain.meta_median(b))\n",
    "                temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "                for i in range(len(temp_meta)):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in temp_meta[i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i+len(importance_test[j])].append(cnzw)\n",
    "                    nzw_scores_delta[i+len(importance_test[j])].append(cnzwt)\n",
    "                my_altruist.fis = len(meta_names)\n",
    "                a = my_altruist.find_untruthful(X_test[j], temp_meta)\n",
    "                for i in range(len(a[0])):\n",
    "                    meta_scores[i].append(len(a[0][i]))\n",
    "            count = 0\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            for meta_score in meta_scores:\n",
    "                row.append(np.array(meta_score).mean())\n",
    "            with open('D2_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names+meta_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D2_NZW_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to produce $f$ atom type arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_altruist = Altruist(neural_type, X_train, fi_techniques, feature_names,\n",
    "                       level=noise, delta=delta, args=True)  # Enable args=True\n",
    "# After running the find_untruthful method, the last element of the returned elements are the arguments for each interpretation technqiue\n",
    "a = my_altruist.find_untruthful(X_test[j], importance_test[j])\n",
    "arguments = a[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore one argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The alteration (INC) did not happen, as the feature value had the max value',\n",
       " \"$f_{car, DEC}$: The evaluation of the alteration of $car$'s value to $0.9903$ ($DEC$) was performed and the model's behaviour was as expected $Increased$ (0.4386 to 0.4442), according to its importance $z_{car}=-0.63$.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments[0][1]  # 0 indicates the interpretation technique, and 1 the feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for NN\n",
      "Results for: Noise -> normal  Delta -> 0.0001\n",
      "\t NZW:\n",
      "Starting evaluation for cNN\n",
      "Results for: Noise -> normal  Delta -> 0.0001\n",
      "\t NZW:\n"
     ]
    }
   ],
   "source": [
    "predict_functions = {'NN': predict_NN, 'cNN': predict_cNN}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'NN':\n",
    "        fi_techniques = [fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D2_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_test = np.array(importances['test'])\n",
    "    meta_explain = MetaExplain(importance_train, feature_names)\n",
    "    importance_train = np.delete(importance_train, 3, axis=1)\n",
    "    importance_test = np.delete(importance_test, 3, axis=1)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['normal']:\n",
    "        for delta in [0.0001]:\n",
    "            my_altruist = Altruist(neural_type, X_train, fi_techniques,\n",
    "                                   feature_names, prolog=False, level=noise, delta=delta)\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(X_test)):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(X_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                temp_meta = []\n",
    "                temp_meta.append(meta_explain.meta_avg(b))\n",
    "                temp_meta.append(meta_explain.meta_median(b))\n",
    "                temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "                for i in range(len(temp_meta)):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in temp_meta[i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i+len(importance_test[j])].append(cnzw)\n",
    "                    nzw_scores_delta[i+len(importance_test[j])].append(cnzwt)\n",
    "                my_altruist.fis = len(meta_names)\n",
    "                a = my_altruist.find_untruthful(X_test[j], temp_meta)\n",
    "                for i in range(len(a[0])):\n",
    "                    meta_scores[i].append(len(a[0][i]))\n",
    "            count = 0\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            for meta_score in meta_scores:\n",
    "                row.append(np.array(meta_score).mean())\n",
    "            with open('D2_Ablation_Study_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names+meta_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D2_Ablation_Study_NZW_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd5e8c6fff498abb6f50ff8dbceb932bf4b3fcabb88a01361989616fd97963bd"
  },
  "kernelspec": {
   "display_name": "lionets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
