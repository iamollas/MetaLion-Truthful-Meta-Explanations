{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative experiments with Turbofan Engine Degradation Simulation dataset\n",
    "This dataset contains information was designed to solve predictive maintenance problems. The dataset is accessible here: \n",
    "https://www.kaggle.com/datasets/behrad3d/nasa-cmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New')\n",
    "model_path = 'C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New\\\\experiments\\\\quantitative\\\\Models\\\\D1\\\\'\n",
    "weights_path = 'C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New\\\\experiments\\\\quantitative\\\\Weights\\\\D1\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import warnings\n",
    "import json\n",
    "import lime.lime_tabular as lt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Input, Dropout, LSTM, concatenate, Reshape\n",
    "import json\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from innvestigate.utils.keras import checks\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "from altruist import Altruist\n",
    "from meta_explain import MetaExplain\n",
    "from utilities.dataset import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, maxabs_scale\n",
    "import numpy as np\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load our TEDS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "teds = Dataset()\n",
    "x_train, y_train, x_test, y_test, feature_names = teds.load_data_turbofan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['s_02', 's_03', 's_04', 's_07', 's_08', 's_09', 's_11', 's_12',\n",
    "                 's_13', 's_14', 's_15', 's_17', 's_20', 's_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_y_train = [[i] for i in y_train]\n",
    "temp_y_test = [[i] for i in y_test]\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(temp_y_train)\n",
    "y_train = target_scaler.transform(temp_y_train)\n",
    "y_test = target_scaler.transform(temp_y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will build two neural network models. One linear and one non-linear, and one uneccessary complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_input = Input(shape=(x_train[0].shape))\n",
    "linear_hidden = Flatten()(linear_input)\n",
    "linear_output = Dense(1, activation='linear')(linear_hidden)\n",
    "linear_neural = Model(linear_input, linear_output)\n",
    "linear_neural.compile(optimizer='adam', loss=[\n",
    "                      root_mean_squared_error], metrics=['mae', 'mse'])\n",
    "\n",
    "neural_input = Input(shape=(x_train[0].shape))\n",
    "hidden_input = Reshape((14, 50))(neural_input)\n",
    "neural_r = []\n",
    "for i in range(14):\n",
    "    temp_hidden = LSTM(units=51, dropout=0.5,\n",
    "                       return_sequences=True, activation='tanh')(hidden_input)\n",
    "    temp_hidden = Dropout(0.5)(temp_hidden)\n",
    "    temp_hidden = LSTM(units=50, dropout=0.5,\n",
    "                       return_sequences=False, activation='tanh')(temp_hidden)\n",
    "    neural_r.append(temp_hidden)\n",
    "neural_hidden = concatenate(neural_r)\n",
    "neural_hidden = Dropout(0.5)(neural_hidden)\n",
    "neural_hidden = Dense(500, activation='tanh')(neural_hidden)  # Relu and selu\n",
    "neural_hidden = Dropout(0.5)(neural_hidden)\n",
    "neural_output = Dense(1, activation='linear')(neural_hidden)  # Relu and selu\n",
    "neural = Model(neural_input, neural_output)\n",
    "neural.compile(optimizer='adam', loss=[\n",
    "               root_mean_squared_error], metrics=['mae', 'mse'])\n",
    "\n",
    "complex_neural_input = Input(shape=(x_train[0].shape))\n",
    "complex_input = Reshape((14, 50))(complex_neural_input)\n",
    "complex_r = []\n",
    "for i in range(14):\n",
    "    complex_hidden = LSTM(\n",
    "        units=51, dropout=0.5, return_sequences=True, activation='tanh')(complex_input)\n",
    "    complex_hidden = Dropout(0.5)(complex_hidden)\n",
    "    complex_hidden = LSTM(\n",
    "        units=50, dropout=0.5, return_sequences=False, activation='relu')(complex_hidden)\n",
    "    complex_hidden = Dropout(0.5)(complex_hidden)\n",
    "    complex_hidden = Dense(units=50, activation='relu')(complex_hidden)\n",
    "    complex_r.append(complex_hidden)\n",
    "complex_net = concatenate(complex_r)\n",
    "complex_net = Dropout(0.5)(complex_net)\n",
    "complex_net = Dense(500, activation='tanh')(complex_net)\n",
    "complex_net = concatenate([complex_net, Flatten()(complex_neural_input)])\n",
    "complex_net = Dropout(0.5)(complex_net)\n",
    "complex_net = Dense(1000, activation='sigmoid')(complex_net)\n",
    "complex_net = Dropout(0.5)(complex_net)\n",
    "complex_output = Dense(1, activation='linear')(complex_net)\n",
    "complex_neural = Model(complex_neural_input, complex_output)\n",
    "complex_neural.compile(optimizer='adam', loss=[\n",
    "                       root_mean_squared_error], metrics=['mae', 'mse'])\n",
    "\n",
    "models = {'lNN': linear_neural, 'NN': neural, 'cNN': complex_neural}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "def compute_scores(name, y_test, y_pred):\n",
    "    temp_y_pred = target_scaler.inverse_transform(y_pred)\n",
    "    y_pred = np.array([i[0] for i in temp_y_pred])\n",
    "    print(name)\n",
    "    print('\\t', 'MAE:', mean_absolute_error(temp_y_test, y_pred))\n",
    "    print('\\t', 'MSE:', mean_squared_error(temp_y_test, y_pred))\n",
    "    print('\\t', 'RMSE:', sqrt(mean_squared_error(temp_y_test, y_pred)))\n",
    "    print('\\t', 'R2:', r2_score(temp_y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNN\n",
      "\t MAE: 27.4459676816527\n",
      "\t MSE: 1367.2966335026808\n",
      "\t RMSE: 36.97697436922984\n",
      "\t R2: 0.49828356104652527\n",
      "NN\n",
      "\t MAE: 24.772757201740628\n",
      "\t MSE: 1260.1166593068872\n",
      "\t RMSE: 35.49812191238978\n",
      "\t R2: 0.537612228771599\n",
      "cNN\n",
      "\t MAE: 22.402879480157456\n",
      "\t MSE: 1069.820192553104\n",
      "\t RMSE: 32.70810591509548\n",
      "\t R2: 0.6074396994942856\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "for name, model in models.items():\n",
    "    if train:\n",
    "        check_point = ModelCheckpoint(\n",
    "            \"D1_\"+name+\".hdf5\", monitor=\"val_loss\", verbose=0, save_best_only=True, mode=\"auto\")\n",
    "        model.fit(x_train, y_train, epochs=500, batch_size=512,\n",
    "                  validation_split=0.33, verbose=0, callbacks=[check_point])\n",
    "        model.load_weights(\"D1_\"+name+\".hdf5\")\n",
    "    else:\n",
    "        model.load_weights(model_path+\"D1_\"+name+\".hdf5\")\n",
    "    y_pred = model.predict(x_test)\n",
    "    compute_scores(name, y_test, y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will prepare our predict functions to work well with our python scripts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cNN(x):\n",
    "    prediction = models['cNN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "def predict_NN(x):\n",
    "    prediction = models['NN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "def predict_lNN(x):\n",
    "    prediction = models['lNN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "predict_functions = {'lNN': predict_lNN, 'NN': predict_NN, 'cNN': predict_cNN}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following function we generate Integrated Gradients and LRP explainers for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_generators(model):\n",
    "    Xs = iutils.to_list(model.outputs)\n",
    "    ret = []\n",
    "    for x in Xs:\n",
    "        layer, node_index, tensor_index = x._keras_history\n",
    "        if checks.contains_activation(layer, activation=\"linear\"):\n",
    "            if isinstance(layer, keras.layers.Activation):\n",
    "                ret.append(layer.get_input_at(node_index))\n",
    "            else:\n",
    "                layer_wo_act = innvestigate.utils.keras.graph.copy_layer_wo_activation(\n",
    "                    layer)\n",
    "                ret.append(layer_wo_act(layer.get_input_at(node_index)))\n",
    "    modified_model = Model(input=model.input, output=ret)\n",
    "    modified_model.trainable = False\n",
    "    modified_model.compile(optimizer='adam', loss=[\n",
    "                           root_mean_squared_error], metrics=['mae', 'mse'])\n",
    "    analyzer_IG = innvestigate.create_analyzer(\n",
    "        'integrated_gradients', modified_model, reference_inputs=16*[0])\n",
    "    analyzer_LRP = innvestigate.create_analyzer('lrp.z', modified_model)\n",
    "    return [analyzer_IG, analyzer_LRP]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate these explainers for each of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_NN = analyzer_generators(neural)\n",
    "analyzer_lNN = analyzer_generators(linear_neural)\n",
    "analyzer_cNN = analyzer_generators(complex_neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.copy().reshape((len(x_train), 700))\n",
    "flat_features = [str('F_'+str(i)) for i in range(700)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare each interpretation technique to have the same format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_lNN(instance, predict_function):\n",
    "    return [i[0] for i in models['lNN'].get_weights()[0]]\n",
    "\n",
    "def fi_IG_NN(instance, predict_function):\n",
    "    a = analyzer_NN[0].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_LRP_NN(instance, predict_function):\n",
    "    a = analyzer_NN[1].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_IG_lNN(instance, predict_function):\n",
    "    a = analyzer_lNN[0].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_LRP_lNN(instance, predict_function):\n",
    "    a = analyzer_lNN[1].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_IG_cNN(instance, predict_function):\n",
    "    a = analyzer_cNN[0].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_LRP_cNN(instance, predict_function):\n",
    "    a = analyzer_cNN[1].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "explainer = lt.LimeTabularExplainer(training_data=x_train_flat,\n",
    "                                    feature_names=flat_features,\n",
    "                                    discretize_continuous=False, mode='regression')\n",
    "\n",
    "def fi_lime(instance, predict_function):\n",
    "    def predict(x):\n",
    "        return np.array([i for i in predict_function(x.reshape((len(x), 50, 14)))])\n",
    "    b = explainer.explain_instance(instance.reshape((700)), predict, num_samples=1000,\n",
    "                                   num_features=len(list(flat_features)))[0].local_exp\n",
    "    b = b[list(b.keys())[0]]\n",
    "    b.sort()\n",
    "    return [i[1] for i in list(b)]\n",
    "\n",
    "def fi_random(instance, predict_function):\n",
    "    a1 = instance.reshape((700))\n",
    "    seed = (a1.sum() +\n",
    "            a1.mean())/10\n",
    "    random.seed(seed)\n",
    "    return [random.randrange(-1000, 1000)/1000 for i in range(len(flat_features))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute and save the interpretations for easier reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "\n",
    "    importance_train = []\n",
    "    for instance in x_train:\n",
    "        importance_instance = []\n",
    "        for fi in fi_techniques:\n",
    "            importance_instance.append(maxabs_scale(fi(instance, neural_type)))\n",
    "        importance_train.append(importance_instance)\n",
    "    importance_train = np.array(importance_train)\n",
    "\n",
    "    importance_test = []\n",
    "    for instance in x_test:\n",
    "        importance_instance = []\n",
    "        for fi in fi_techniques:\n",
    "            importance_instance.append(maxabs_scale(fi(instance, neural_type)))\n",
    "        importance_test.append(importance_instance)\n",
    "    importance_test = np.array(importance_test)\n",
    "\n",
    "    importances = {'train': importance_train.tolist(),\n",
    "                   'test': importance_test.tolist()}\n",
    "    with open('D1_'+neural_name+'.txt', 'w') as outfile:\n",
    "        json.dump(importances, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our quantitative experiments!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with stability! First, we calculate stability for the techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inxai import *\n",
    "gm = GlobalFeatureMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNN\n",
      "Inherent 1.0\n",
      "IG 0.5472611448448466\n",
      "LRP 0.5472613508372738\n",
      "LIME 0.8827196335774936\n",
      "RAND 0.1226924686983951\n",
      "NN\n",
      "IG 0.4335826292356411\n",
      "LRP 0.2420931505494108\n",
      "LIME 0.35764355702709877\n",
      "RAND 0.1226924686983951\n",
      "cNN\n",
      "IG 0.44254492157659747\n",
      "LRP 0.1871609435774639\n",
      "LIME 0.3720531362075941\n",
      "RAND 0.1226924686983951\n"
     ]
    }
   ],
   "source": [
    "stability = {}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_test = np.array(importances['test'])\n",
    "    meta_explain = MetaExplain(importance_train, flat_features)\n",
    "\n",
    "    model_stability = []\n",
    "    for idf,fi_name in enumerate(fi_names):\n",
    "        model_stability.append(gm.stability(pd.DataFrame(x_test.reshape((len(x_test),700))),\n",
    "                                            importance_test[:,idf,:] ,epsilon=3))\n",
    "    stability[neural_name] = model_stability\n",
    "    \n",
    "    print(neural_name)\n",
    "    for idf, stability_score in enumerate(stability[neural_name]):\n",
    "        print(fi_names[idf], np.mean(stability_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for lNN\n",
      "Average 0.4085841105718534\n",
      "Median 0.5331231712395894\n",
      "RuleBased 0.5236872470424525\n",
      "Starting evaluation for NN\n",
      "Average 0.3408131490683045\n",
      "Median 0.3394097448598858\n",
      "RuleBased 0.37873233891676067\n",
      "Starting evaluation for cNN\n",
      "Average 0.3217044803567875\n",
      "Median 0.29577670762027564\n",
      "RuleBased 0.3909713871399417\n"
     ]
    }
   ],
   "source": [
    "meta_interpretations_test = {}\n",
    "stability_meta = {}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "        meta_interpretations_test['lNN'] = []\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "        meta_interpretations_test['NN'] = []\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "        meta_interpretations_test['cNN'] = []\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_test = np.array(importances['test'])\n",
    "    meta_explain = MetaExplain(importance_train, flat_features)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    noise = 'normal'\n",
    "    delta = 0.0001\n",
    "\n",
    "    my_altruist = Altruist(neural_type, x_train, fi_techniques,\n",
    "                                   flat_features, level=noise, delta=delta)\n",
    "    for j in range(len(x_test)):\n",
    "        my_altruist.fis = len(fi_techniques)\n",
    "        a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "        b = np.array(a[-1])\n",
    "\n",
    "        temp_meta = []\n",
    "        temp_meta.append(meta_explain.meta_avg(b))\n",
    "        temp_meta.append(meta_explain.meta_median(b))\n",
    "        temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "        meta_interpretations_test[neural_name].append(temp_meta)\n",
    "    \n",
    "    model_stability = []\n",
    "    for idf,meta_name in enumerate(meta_names):\n",
    "        model_stability.append(gm.stability(pd.DataFrame(x_test.reshape((len(x_test),700))),\n",
    "        np.array(meta_interpretations_test[neural_name])[:,idf,:] ,epsilon=3))\n",
    "    stability_meta[neural_name] = model_stability\n",
    "\n",
    "    for idf, stability_score in enumerate(stability_meta[neural_name]):\n",
    "        print(meta_names[idf], np.mean(stability_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_functions = {'lNN': predict_lNN, 'NN': predict_NN, 'cNN': predict_cNN}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_test = np.array(importances['test'])\n",
    "    meta_explain = MetaExplain(importance_train, flat_features)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['weak', 'normal', 'strong']:\n",
    "        for delta in [0, 0.0001, 0.001, 0.01]:\n",
    "            my_altruist = Altruist(neural_type, x_train, fi_techniques,\n",
    "                                   flat_features, level=noise, delta=delta)\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(x_test)):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                temp_meta = []\n",
    "                temp_meta.append(meta_explain.meta_avg(b))\n",
    "                temp_meta.append(meta_explain.meta_median(b))\n",
    "                temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "                for i in range(len(temp_meta)):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in temp_meta[i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i+len(importance_test[j])].append(cnzw)\n",
    "                    nzw_scores_delta[i+len(importance_test[j])].append(cnzwt)\n",
    "                my_altruist.fis = len(meta_names)\n",
    "                a = my_altruist.find_untruthful(x_test[j], temp_meta)\n",
    "                for i in range(len(a[0])):\n",
    "                    meta_scores[i].append(len(a[0][i]))\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            for meta_score in meta_scores:\n",
    "                row.append(np.array(meta_score).mean())\n",
    "            with open('D1_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names+meta_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D1_NZW_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the interpretations on the Sensor level as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [30:22, 364.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNN\n",
      "Inherent 1.0\n",
      "IG 0.7754069242128709\n",
      "LRP 0.7754069925125184\n",
      "LIME 0.9768406586793216\n",
      "RAND 0.6236217689799591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [28:52, 433.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "IG 0.7398262757875596\n",
      "LRP 0.5604877958811235\n",
      "LIME 0.8696876084631823\n",
      "RAND 0.6236217689799591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [28:41, 430.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cNN\n",
      "IG 0.796563454172929\n",
      "LRP 0.6788027322776082\n",
      "LIME 0.8775863495137214\n",
      "RAND 0.6236217689799591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "stability = {}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_test = np.array(importances['test'])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_test[:1000]:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "\n",
    "    sensor_names = [str('F_'+str(i)) for i in range(14)]\n",
    "\n",
    "    #meta_explain = MetaExplain(importance_train, sensor_names)\n",
    "\n",
    "    model_stability = []\n",
    "    for idf,fi_name in tqdm(enumerate(fi_names)):\n",
    "        model_stability.append(gm.stability(pd.DataFrame(x_test[:1000].mean(axis=1)),\n",
    "                                            importance_test[:,idf,:] ,epsilon=3))\n",
    "    stability[neural_name] = model_stability\n",
    "    \n",
    "    print(neural_name)\n",
    "    for idf, stability_score in enumerate(stability[neural_name]):\n",
    "        print(fi_names[idf], np.mean(stability_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for lNN\n",
      "Average 0.8730950784328496\n",
      "Median 0.875125988379947\n",
      "RuleBased 0.6637796813569781\n",
      "Starting evaluation for NN\n",
      "Average 0.7950506213069923\n",
      "Median 0.8020430809623137\n",
      "RuleBased 0.723609150373512\n",
      "Starting evaluation for cNN\n",
      "Average 0.8441905393598813\n",
      "Median 0.8272401586232013\n",
      "RuleBased 0.7946388878535836\n"
     ]
    }
   ],
   "source": [
    "meta_interpretations_test = {}\n",
    "stability_meta = {}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "        meta_interpretations_test['lNN'] = []\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "        meta_interpretations_test['NN'] = []\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "        meta_interpretations_test['cNN'] = []\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_train_temp = []\n",
    "    importance_test = np.array(importances['test'])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_train:\n",
    "        importance_train_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    for i in importance_test[:1000]:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_train = np.array(importance_train_temp)\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    noise = 'normal'\n",
    "    delta = 0.0001\n",
    "    sensor_names = [str('F_'+str(i)) for i in range(14)]\n",
    "\n",
    "    meta_explain = MetaExplain(importance_train, sensor_names)\n",
    "\n",
    "    my_altruist = Altruist(neural_type, x_train, fi_techniques,\n",
    "                                   sensor_names, level=noise, delta=delta, data_type='per Sensor')\n",
    "    for j in range(len(x_test[:1000])):\n",
    "        my_altruist.fis = len(fi_techniques)\n",
    "        a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "        b = np.array(a[-1])\n",
    "\n",
    "        temp_meta = []\n",
    "        temp_meta.append(meta_explain.meta_avg(b))\n",
    "        temp_meta.append(meta_explain.meta_median(b))\n",
    "        temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "        meta_interpretations_test[neural_name].append(temp_meta)\n",
    "    \n",
    "    model_stability = []\n",
    "    for idf,meta_name in enumerate(meta_names):\n",
    "        model_stability.append(gm.stability(pd.DataFrame(x_test[:1000].mean(axis=1)),\n",
    "        np.array(meta_interpretations_test[neural_name])[:,idf,:] ,epsilon=3))\n",
    "    stability_meta[neural_name] = model_stability\n",
    "\n",
    "    for idf, stability_score in enumerate(stability_meta[neural_name]):\n",
    "        print(meta_names[idf], np.mean(stability_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25986346a88>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "lip_df = pd.DataFrame({'IG':stability['cNN'][0], 'LRP':stability['cNN'][1], 'LIME':stability['cNN'][2], # 'RAND':stability['cNN'][3], \n",
    "                        'Average':stability_meta['cNN'][0], 'Median':stability_meta['cNN'][1], 'MetaLion':stability_meta['cNN'][2]})\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(lip_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_df_lnn = pd.DataFrame({'IG':stability['lNN'][1], 'LRP':stability['lNN'][2], 'LIME':stability['lNN'][3], 'RAND':stability['lNN'][4]})\n",
    "lip_df_nn = pd.DataFrame({'IG':stability['NN'][0], 'LRP':stability['NN'][1], 'LIME':stability['NN'][2], 'RAND':stability['NN'][3]})\n",
    "lip_df_cnn = pd.DataFrame({'IG':stability['cNN'][0], 'LRP':stability['cNN'][1], 'LIME':stability['cNN'][2], 'RAND':stability['cNN'][3]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure consistency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG 0.11258650447913647\n",
      "LRP 0.0871944291161526\n",
      "LIME 0.10901834747484684\n",
      "RAND 1.0\n"
     ]
    }
   ],
   "source": [
    "consistency = {}\n",
    "for idf,fi_name in enumerate(['IG', 'LRP', 'LIME', 'RAND']):\n",
    "    all_importance_tests = []\n",
    "    for neural_name, neural_type in predict_functions.items():\n",
    "\n",
    "        with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "            importances = json.load(json_file)\n",
    "        importance_test_temp = []\n",
    "        importance_test = np.array(importances['test'])\n",
    "        for i in importance_test[:1000]:\n",
    "            importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "        importance_test = np.array(importance_test_temp)\n",
    "        if neural_name == 'lNN':\n",
    "            all_importance_tests.append(np.array(importances['test'])[:1000,1:,:])\n",
    "        else:\n",
    "            all_importance_tests.append(np.array(importances['test'][:1000]))\n",
    "        #meta_explain = MetaExplain(importance_train, feature_names)\n",
    "    all_importance_tests = np.array(all_importance_tests)\n",
    "    consistency[fi_name] = gm.consistency(all_importance_tests[:,:,idf,:])\n",
    "    print(fi_name, np.mean(consistency[fi_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 0.7706064399744632\n",
      "Median 0.6348928652997506\n",
      "RuleBased 0.6435878756302303\n"
     ]
    }
   ],
   "source": [
    "consistency_meta = {}\n",
    "for idf, meta_name in enumerate(meta_names):\n",
    "    all_importance_tests = []\n",
    "    for neural_name, neural_type in predict_functions.items():\n",
    "        all_importance_tests.append(np.array(meta_interpretations_test[neural_name]))\n",
    "    all_importance_tests = np.array(all_importance_tests)\n",
    "    consistency_meta[meta_name] = gm.consistency(all_importance_tests[:,:,idf,:])\n",
    "    print(meta_name, np.mean(consistency_meta[meta_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(consistency['IG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25ac75bde48>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "con_df = pd.DataFrame({'IG':consistency['IG'], 'LRP':consistency['LRP'], 'LIME':consistency['LIME'], #'RAND':consistency['RAND'], \n",
    "                        'Average':consistency_meta['Average'], 'Median':consistency_meta['Median'], 'MetaLion':consistency_meta['RuleBased']})\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(con_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "con_df_models = pd.DataFrame({'IG':consistency['IG'], 'LRP':consistency['LRP'], 'LIME':consistency['LIME'], 'RAND':consistency['RAND']})\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This is a tough one! Measuring AUPRC for the meta in inXAI technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5, 700)\n",
      "Starting evaluation for lNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [08:28, 101.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4, 700)\n",
      "Starting evaluation for NN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:20:58, 1214.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4, 700)\n",
      "Starting evaluation for cNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:31:44, 1376.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "pd_x_test = pd.DataFrame(x_test[:1000].reshape((len(x_test[:1000]),700)), columns=flat_features)\n",
    "ct = ColumnTransformer([('_INXAI_normal_noise_perturber', NormalNoisePerturber(scale=2),pd_x_test.columns)])\n",
    "auprc = {}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_test = np.array(importances['test'])[:1000]\n",
    "    print(importance_test.shape)\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    model_auprc = []\n",
    "    for idf,fi_name in tqdm(enumerate(fi_names)):\n",
    "        mean_fi = importance_test[:,idf,:].mean(axis=0)\n",
    "        model_auprc.append(gm.gradual_perturbation(model=models[neural_name], X=pd_x_test, y=y_test[:1000], column_transformer=ct, importances_orig=mean_fi, \n",
    "                                        resolution=50,  count_per_step=10, plot=False, task='r', reshape=(50,14)))\n",
    "    auprc[neural_name] = model_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNN\n",
      "Inherent 198.78001759133895\n",
      "IG 198.72447586550163\n",
      "LRP 200.1155860640735\n",
      "LIME 201.2579751175869\n",
      "RAND 199.44617678597345\n",
      "NN\n",
      "IG 18.61691716734181\n",
      "LRP 18.68428245110465\n",
      "LIME 18.717886467914365\n",
      "RAND 18.677940928682705\n",
      "cNN\n",
      "IG 9.180022796778584\n",
      "LRP 9.277766039359863\n",
      "LIME 9.22172752775051\n",
      "RAND 9.282237353272976\n"
     ]
    }
   ],
   "source": [
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    print(neural_name)\n",
    "    for idf, auprc_score in enumerate(auprc[neural_name]):\n",
    "        print(fi_names[idf], np.mean(auprc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "auprc_df_lnn = pd.DataFrame({'IG':[auc(np.linspace(0, 1, 50),auprc['lNN'][1])], 'LRP':[auc(np.linspace(0, 1, 50),auprc['lNN'][2])], 'LIME':[auc(np.linspace(0, 1, 50),auprc['lNN'][3])], 'RAND':[auc(np.linspace(0, 1, 50),auprc['lNN'][4])]})\n",
    "auprc_df_nn = pd.DataFrame({'IG':[auc(np.linspace(0, 1, 50),auprc['NN'][0])], 'LRP':[auc(np.linspace(0, 1, 50),auprc['NN'][1])], 'LIME':[auc(np.linspace(0, 1, 50),auprc['NN'][2])], 'RAND':[auc(np.linspace(0, 1, 50),auprc['NN'][3])]})\n",
    "auprc_df_cnn = pd.DataFrame({'IG':[auc(np.linspace(0, 1, 50),auprc['cNN'][0])], 'LRP':[auc(np.linspace(0, 1, 50),auprc['cNN'][1])], 'LIME':[auc(np.linspace(0, 1, 50),auprc['cNN'][2])], 'RAND':[auc(np.linspace(0, 1, 50),auprc['cNN'][3])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_alpha = 0.2\n",
    "lip_alpha=20\n",
    "auc_alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IG</th>\n",
       "      <th>LRP</th>\n",
       "      <th>LIME</th>\n",
       "      <th>RAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.314669</td>\n",
       "      <td>15.313417</td>\n",
       "      <td>19.640208</td>\n",
       "      <td>12.639863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IG        LRP       LIME       RAND\n",
       "0  15.314669  15.313417  19.640208  12.639863"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cons_alpha*con_df_models.iloc[33]+lip_alpha*lip_df_lnn.iloc[33]+auc_alpha/auprc_df_lnn\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for lNN\n",
      "inXAI 0.7950475634943016\n",
      "Starting evaluation for NN\n",
      "inXAI 0.7607737064929643\n",
      "Starting evaluation for cNN\n",
      "inXAI 0.796306129274939\n"
     ]
    }
   ],
   "source": [
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_test = np.array(importances['test'])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_test[:1000]:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "\n",
    "    inxai = []\n",
    "    for idf, instance in enumerate(x_test[:1000]):\n",
    "        if neural_name == 'lNN':\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_lnn.iloc[idf]+auc_alpha/auprc_df_lnn\n",
    "            met = weights.dot(importance_test[idf][1:])/weights.sum().sum()\n",
    "        elif neural_name == 'NN':\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_nn.iloc[idf]+auc_alpha/auprc_df_nn\n",
    "            met = weights.dot(importance_test[idf])/weights.sum().sum()\n",
    "        else:\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_cnn.iloc[idf]+auc_alpha/auprc_df_cnn\n",
    "            met = weights.dot(importance_test[idf])/weights.sum().sum()\n",
    "        inxai.append(met)\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    inxai_stability = gm.stability(pd.DataFrame(x_test[:1000].mean(axis=1)), inxai ,epsilon=3)\n",
    "    print('inXAI', np.mean(inxai_stability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inXAI 0.8922154541544404\n"
     ]
    }
   ],
   "source": [
    "all_importance_tests = []\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "        importance_test_temp = []\n",
    "        importance_test = np.array(importances['test'])\n",
    "        for i in importance_test[:1000]:\n",
    "            importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "        importance_test = np.array(importance_test_temp)\n",
    "    if neural_name == 'lNN':\n",
    "        per_neural = np.array(importances['test'])[:1000,1:,:]\n",
    "    else:\n",
    "        per_neural = np.array(importances['test'][:1000])\n",
    "    \n",
    "    for idf, instance in enumerate(x_test[:1000]):\n",
    "        if neural_name == 'lNN':\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_lnn.iloc[idf]+auc_alpha/auprc_df_lnn\n",
    "            met = weights.dot(per_neural[idf])/weights.sum().sum()\n",
    "        elif neural_name == 'NN':\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_nn.iloc[idf]+auc_alpha/auprc_df_nn\n",
    "            met = weights.dot(per_neural[idf])/weights.sum().sum()\n",
    "        else:\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_cnn.iloc[idf]+auc_alpha/auprc_df_cnn\n",
    "            met = weights.dot(per_neural[idf])/weights.sum().sum()\n",
    "    all_importance_tests.append(met)\n",
    "all_importance_tests = np.array(all_importance_tests)\n",
    "inxai_consistency = gm.consistency(all_importance_tests)\n",
    "print('inXAI', np.mean(inxai_consistency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truthfulness and Commplexity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_functions = {'lNN': predict_lNN, 'NN': predict_NN, 'cNN': predict_cNN}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_train_temp = []\n",
    "    importance_test = np.array(importances['test'])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_train:\n",
    "        importance_train_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    for i in importance_test:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_train = np.array(importance_train_temp)\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "\n",
    "    sensor_names = [str('F_'+str(i)) for i in range(14)]\n",
    "\n",
    "    meta_explain = MetaExplain(importance_train, sensor_names)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['weak', 'normal', 'strong']:\n",
    "        for delta in [0, 0.0001, 0.001, 0.01, 0.1]:\n",
    "            my_altruist = Altruist(neural_type, x_train, fi_techniques, sensor_names,\n",
    "                                   level=noise, delta=delta, data_type='per Sensor')\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(x_test[:1000])):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                temp_meta = []\n",
    "                temp_meta.append(meta_explain.meta_avg(b))\n",
    "                temp_meta.append(meta_explain.meta_median(b))\n",
    "                temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "                for i in range(len(temp_meta)):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in temp_meta[i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i+len(importance_test[j])].append(cnzw)\n",
    "                    nzw_scores_delta[i+len(importance_test[j])].append(cnzwt)\n",
    "                my_altruist.fis = len(meta_names)\n",
    "                a = my_altruist.find_untruthful(x_test[j], temp_meta)\n",
    "                for i in range(len(a[0])):\n",
    "                    meta_scores[i].append(len(a[0][i]))\n",
    "\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            for meta_score in meta_scores:\n",
    "                row.append(np.array(meta_score).mean())\n",
    "            with open('D1_PS'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names+meta_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D1_PS_NZW_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for lNN\n",
      "Starting evaluation for NN\n",
      "Starting evaluation for cNN\n"
     ]
    }
   ],
   "source": [
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_random]\n",
    "        fi_names = ['inXAI']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_random]\n",
    "        fi_names = ['inXAI']\n",
    "    else:\n",
    "        fi_techniques = [fi_random]\n",
    "        fi_names = ['inXAI']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_train_temp = []\n",
    "    importance_test = np.array(importances['test'][:1000])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_train:\n",
    "        importance_train_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    for i in importance_test:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_train = np.array(importance_train_temp)\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "\n",
    "    new_test = []\n",
    "    for idf, instance in enumerate(x_test[:1000]):\n",
    "        if neural_name == 'lNN':\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_lnn.iloc[idf]+auc_alpha/auprc_df_lnn\n",
    "            met = weights.dot(importance_test[idf][1:])/weights.sum().sum()\n",
    "        elif neural_name == 'NN':\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_nn.iloc[idf]+auc_alpha/auprc_df_nn\n",
    "            met = weights.dot(importance_test[idf])/weights.sum().sum()\n",
    "        else:\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_cnn.iloc[idf]+auc_alpha/auprc_df_cnn\n",
    "            met = weights.dot(importance_test[idf])/weights.sum().sum()\n",
    "        new_test.append([met])\n",
    "    importance_test = np.array(new_test)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['weak', 'normal', 'strong']:\n",
    "        for delta in [0 , 0.0001, 0.001, 0.01, 0.1]:\n",
    "            my_altruist = Altruist(\n",
    "                neural_type, x_train, fi_techniques, sensor_names, level=noise, delta=delta, data_type='per Sensor')\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(x_test[:1000])):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                \n",
    "            count = 0\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            with open('D1_inxai'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D1_NZW_inxai'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will perform the ablation study in per sensor level!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_functions = {'NN': predict_NN, 'cNN': predict_cNN}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'NN':\n",
    "        fi_techniques = [fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['LRP', 'LIME', 'RAND']  # ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['LRP', 'LIME', 'RAND']  # ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_train_temp = []\n",
    "    importance_test = np.array(importances['test'])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_train:\n",
    "        importance_train_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    for i in importance_test:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_train = np.array(importance_train_temp)\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "    importance_train = np.delete(importance_train, 2, axis=1)\n",
    "    importance_test = np.delete(importance_test, 2, axis=1)\n",
    "\n",
    "    sensor_names = [str('F_'+str(i)) for i in range(14)]\n",
    "\n",
    "    meta_explain = MetaExplain(importance_train, sensor_names)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['normal']:\n",
    "        for delta in [0.0001]:\n",
    "            my_altruist = Altruist(neural_type, x_train, fi_techniques, sensor_names,\n",
    "                                   level=noise, delta=delta, data_type='per Sensor')\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(x_test)):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                temp_meta = []\n",
    "                temp_meta.append(meta_explain.meta_avg(b))\n",
    "                temp_meta.append(meta_explain.meta_median(b))\n",
    "                temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "                for i in range(len(temp_meta)):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in temp_meta[i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i+len(importance_test[j])].append(cnzw)\n",
    "                    nzw_scores_delta[i+len(importance_test[j])].append(cnzwt)\n",
    "                my_altruist.fis = len(meta_names)\n",
    "                a = my_altruist.find_untruthful(x_test[j], temp_meta)\n",
    "                for i in range(len(a[0])):\n",
    "                    meta_scores[i].append(len(a[0][i]))\n",
    "\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            for meta_score in meta_scores:\n",
    "                row.append(np.array(meta_score).mean())\n",
    "            with open('D1_PS_AblationIG'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names+meta_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D1_PS_AblationIG_NZW_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for NN\n",
      "Starting evaluation for cNN\n"
     ]
    }
   ],
   "source": [
    "predict_functions = {'NN': predict_NN, 'cNN': predict_cNN}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'NN':\n",
    "        fi_techniques = [fi_random]\n",
    "        fi_names = ['inXAI']\n",
    "    else:\n",
    "        fi_techniques = [fi_random]\n",
    "        fi_names = ['inXAI']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_test = np.array(importances['test'])\n",
    "    \n",
    "    importance_train_temp = []\n",
    "    importance_test = np.array(importances['test'])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_train:\n",
    "        importance_train_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    for i in importance_test[:1000]:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_train = np.array(importance_train_temp)\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "\n",
    "    fi = 3\n",
    "    importance_train =np.delete(importance_train, fi, axis=1) #change this manually\n",
    "    importance_test = np.delete(importance_test, fi, axis=1)\n",
    "    new_test = []\n",
    "    for idf, instance in enumerate(x_test[:1000]):\n",
    "        if neural_name == 'NN':\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_nn.iloc[idf]+auc_alpha/auprc_df_nn\n",
    "            met = weights.drop(weights.columns[[fi]],axis = 1).dot(importance_test[idf])/weights.sum().sum()\n",
    "        else:\n",
    "            weights = cons_alpha*con_df_models.iloc[idf]+lip_alpha*lip_df_cnn.iloc[idf]+auc_alpha/auprc_df_cnn\n",
    "            met = weights.drop(weights.columns[[fi]],axis = 1).dot(importance_test[idf])/weights.sum().sum()\n",
    "        new_test.append([met])\n",
    "    importance_test = np.array(new_test)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['normal']:\n",
    "        for delta in [0.0001]:\n",
    "            my_altruist = Altruist(\n",
    "                neural_type, x_train, fi_techniques, sensor_names, level=noise, delta=delta, data_type='per Sensor')\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(x_test[:1000])):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                \n",
    "            count = 0\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            with open('D1_Ablation_Study_inxai'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D1_Ablation_Study_NZW_inxai'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f35bf8cd5f04ba351bb361a1212eeb730a988adc473d8146210fc49c7facefd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
