{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative experiments with Turbofan Engine Degradation Simulation dataset\n",
    "This dataset contains information was designed to solve predictive maintenance problems. The dataset is accessible here: \n",
    "https://www.kaggle.com/datasets/behrad3d/nasa-cmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New')\n",
    "model_path = 'C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New\\\\experiments\\\\quantitative\\\\Models\\\\D1\\\\'\n",
    "weights_path = 'C:\\\\Users\\\\iamollas\\\\Desktop\\\\Altruist New\\\\experiments\\\\quantitative\\\\Weights\\\\D1\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "import warnings\n",
    "import json\n",
    "import lime.lime_tabular as lt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Input, Dropout, LSTM, concatenate, Reshape\n",
    "import json\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from innvestigate.utils.keras import checks\n",
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "from altruist import Altruist\n",
    "from meta_explain import MetaExplain\n",
    "from utilities.dataset import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, maxabs_scale\n",
    "import numpy as np\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load our TEDS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teds = Dataset()\n",
    "x_train, y_train, x_test, y_test, feature_names = teds.load_data_turbofan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['s_02', 's_03', 's_04', 's_07', 's_08', 's_09', 's_11', 's_12',\n",
    "                 's_13', 's_14', 's_15', 's_17', 's_20', 's_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_y_train = [[i] for i in y_train]\n",
    "temp_y_test = [[i] for i in y_test]\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(temp_y_train)\n",
    "y_train = target_scaler.transform(temp_y_train)\n",
    "y_test = target_scaler.transform(temp_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will build two neural network models. One linear and one non-linear, and one uneccessary complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\iamollas\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "linear_input = Input(shape=(x_train[0].shape))\n",
    "linear_hidden = Flatten()(linear_input)\n",
    "linear_output = Dense(1, activation='linear')(linear_hidden)\n",
    "linear_neural = Model(linear_input, linear_output)\n",
    "linear_neural.compile(optimizer='adam', loss=[\n",
    "                      root_mean_squared_error], metrics=['mae', 'mse'])\n",
    "\n",
    "neural_input = Input(shape=(x_train[0].shape))\n",
    "hidden_input = Reshape((14, 50))(neural_input)\n",
    "neural_r = []\n",
    "for i in range(14):\n",
    "    temp_hidden = LSTM(units=51, dropout=0.5,\n",
    "                       return_sequences=True, activation='tanh')(hidden_input)\n",
    "    temp_hidden = Dropout(0.5)(temp_hidden)\n",
    "    temp_hidden = LSTM(units=50, dropout=0.5,\n",
    "                       return_sequences=False, activation='tanh')(temp_hidden)\n",
    "    neural_r.append(temp_hidden)\n",
    "neural_hidden = concatenate(neural_r)\n",
    "neural_hidden = Dropout(0.5)(neural_hidden)\n",
    "neural_hidden = Dense(500, activation='tanh')(neural_hidden)  # Relu and selu\n",
    "neural_hidden = Dropout(0.5)(neural_hidden)\n",
    "neural_output = Dense(1, activation='linear')(neural_hidden)  # Relu and selu\n",
    "neural = Model(neural_input, neural_output)\n",
    "neural.compile(optimizer='adam', loss=[\n",
    "               root_mean_squared_error], metrics=['mae', 'mse'])\n",
    "\n",
    "complex_neural_input = Input(shape=(x_train[0].shape))\n",
    "complex_input = Reshape((14, 50))(complex_neural_input)\n",
    "complex_r = []\n",
    "for i in range(14):\n",
    "    complex_hidden = LSTM(\n",
    "        units=51, dropout=0.5, return_sequences=True, activation='tanh')(complex_input)\n",
    "    complex_hidden = Dropout(0.5)(complex_hidden)\n",
    "    complex_hidden = LSTM(\n",
    "        units=50, dropout=0.5, return_sequences=False, activation='relu')(complex_hidden)\n",
    "    complex_hidden = Dropout(0.5)(complex_hidden)\n",
    "    complex_hidden = Dense(units=50, activation='relu')(complex_hidden)\n",
    "    complex_r.append(complex_hidden)\n",
    "complex_net = concatenate(complex_r)\n",
    "complex_net = Dropout(0.5)(complex_net)\n",
    "complex_net = Dense(500, activation='tanh')(complex_net)\n",
    "complex_net = concatenate([complex_net, Flatten()(complex_neural_input)])\n",
    "complex_net = Dropout(0.5)(complex_net)\n",
    "complex_net = Dense(1000, activation='sigmoid')(complex_net)\n",
    "complex_net = Dropout(0.5)(complex_net)\n",
    "complex_output = Dense(1, activation='linear')(complex_net)\n",
    "complex_neural = Model(complex_neural_input, complex_output)\n",
    "complex_neural.compile(optimizer='adam', loss=[\n",
    "                       root_mean_squared_error], metrics=['mae', 'mse'])\n",
    "\n",
    "models = {'lNN': linear_neural, 'NN': neural, 'cNN': complex_neural}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "def compute_scores(name, y_test, y_pred):\n",
    "    temp_y_pred = target_scaler.inverse_transform(y_pred)\n",
    "    y_pred = np.array([i[0] for i in temp_y_pred])\n",
    "    print(name)\n",
    "    print('\\t', 'MAE:', mean_absolute_error(temp_y_test, y_pred))\n",
    "    print('\\t', 'MSE:', mean_squared_error(temp_y_test, y_pred))\n",
    "    print('\\t', 'RMSE:', sqrt(mean_squared_error(temp_y_test, y_pred)))\n",
    "    print('\\t', 'R2:', r2_score(temp_y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\iamollas\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "lNN\n",
      "\t MAE: 27.4459676816527\n",
      "\t MSE: 1367.2966335026808\n",
      "\t RMSE: 36.97697436922984\n",
      "\t R2: 0.49828356104652527\n",
      "NN\n",
      "\t MAE: 24.772757201740628\n",
      "\t MSE: 1260.1166593068872\n",
      "\t RMSE: 35.49812191238978\n",
      "\t R2: 0.537612228771599\n",
      "cNN\n",
      "\t MAE: 22.402879480157456\n",
      "\t MSE: 1069.820192553104\n",
      "\t RMSE: 32.70810591509548\n",
      "\t R2: 0.6074396994942856\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "for name, model in models.items():\n",
    "    if train:\n",
    "        check_point = ModelCheckpoint(\n",
    "            \"D1_\"+name+\".hdf5\", monitor=\"val_loss\", verbose=0, save_best_only=True, mode=\"auto\")\n",
    "        model.fit(x_train, y_train, epochs=500, batch_size=512,\n",
    "                  validation_split=0.33, verbose=0, callbacks=[check_point])\n",
    "        model.load_weights(\"D1_\"+name+\".hdf5\")\n",
    "    else:\n",
    "        model.load_weights(model_path+\"D1_\"+name+\".hdf5\")\n",
    "    y_pred = model.predict(x_test)\n",
    "    compute_scores(name, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will prepare our predict functions to work well with our python scripts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cNN(x):\n",
    "    prediction = models['cNN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "def predict_NN(x):\n",
    "    prediction = models['NN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "def predict_lNN(x):\n",
    "    prediction = models['lNN'].predict(x)\n",
    "    return [i[0] for i in prediction]\n",
    "\n",
    "predict_functions = {'lNN': predict_lNN, 'NN': predict_NN, 'cNN': predict_cNN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following function we generate Integrated Gradients and LRP explainers for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer_generators(model):\n",
    "    Xs = iutils.to_list(model.outputs)\n",
    "    ret = []\n",
    "    for x in Xs:\n",
    "        layer, node_index, tensor_index = x._keras_history\n",
    "        if checks.contains_activation(layer, activation=\"linear\"):\n",
    "            if isinstance(layer, keras.layers.Activation):\n",
    "                ret.append(layer.get_input_at(node_index))\n",
    "            else:\n",
    "                layer_wo_act = innvestigate.utils.keras.graph.copy_layer_wo_activation(\n",
    "                    layer)\n",
    "                ret.append(layer_wo_act(layer.get_input_at(node_index)))\n",
    "    modified_model = Model(input=model.input, output=ret)\n",
    "    modified_model.trainable = False\n",
    "    modified_model.compile(optimizer='adam', loss=[\n",
    "                           root_mean_squared_error], metrics=['mae', 'mse'])\n",
    "    analyzer_IG = innvestigate.create_analyzer(\n",
    "        'integrated_gradients', modified_model, reference_inputs=16*[0])\n",
    "    analyzer_LRP = innvestigate.create_analyzer('lrp.z', modified_model)\n",
    "    return [analyzer_IG, analyzer_LRP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate these explainers for each of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_NN = analyzer_generators(neural)\n",
    "analyzer_lNN = analyzer_generators(linear_neural)\n",
    "analyzer_cNN = analyzer_generators(complex_neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.copy().reshape((len(x_train), 700))\n",
    "flat_features = [str('F_'+str(i)) for i in range(700)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare each interpretation technique to have the same format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_lNN(instance, predict_function):\n",
    "    return [i[0] for i in models['lNN'].get_weights()[0]]\n",
    "\n",
    "def fi_IG_NN(instance, predict_function):\n",
    "    a = analyzer_NN[0].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_LRP_NN(instance, predict_function):\n",
    "    a = analyzer_NN[1].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_IG_lNN(instance, predict_function):\n",
    "    a = analyzer_lNN[0].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_LRP_lNN(instance, predict_function):\n",
    "    a = analyzer_lNN[1].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_IG_cNN(instance, predict_function):\n",
    "    a = analyzer_cNN[0].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "def fi_LRP_cNN(instance, predict_function):\n",
    "    a = analyzer_cNN[1].analyze(np.array([instance]))[0]\n",
    "    a = a.reshape((700))\n",
    "    return a.tolist()\n",
    "\n",
    "explainer = lt.LimeTabularExplainer(training_data=x_train_flat,\n",
    "                                    feature_names=flat_features,\n",
    "                                    discretize_continuous=False, mode='regression')\n",
    "\n",
    "def fi_lime(instance, predict_function):\n",
    "    def predict(x):\n",
    "        return np.array([i for i in predict_function(x.reshape((len(x), 50, 14)))])\n",
    "    b = explainer.explain_instance(instance.reshape((700)), predict, num_samples=1000,\n",
    "                                   num_features=len(list(flat_features)))[0].local_exp\n",
    "    b = b[list(b.keys())[0]]\n",
    "    b.sort()\n",
    "    return [i[1] for i in list(b)]\n",
    "\n",
    "def fi_random(instance, predict_function):\n",
    "    a1 = instance.reshape((700))\n",
    "    seed = (a1.sum() +\n",
    "            a1.mean())/10\n",
    "    random.seed(seed)\n",
    "    return [random.randrange(-1000, 1000)/1000 for i in range(len(flat_features))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute and save the interpretations for easier reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "\n",
    "    importance_train = []\n",
    "    for instance in x_train:\n",
    "        importance_instance = []\n",
    "        for fi in fi_techniques:\n",
    "            importance_instance.append(maxabs_scale(fi(instance, neural_type)))\n",
    "        importance_train.append(importance_instance)\n",
    "    importance_train = np.array(importance_train)\n",
    "\n",
    "    importance_test = []\n",
    "    for instance in x_test:\n",
    "        importance_instance = []\n",
    "        for fi in fi_techniques:\n",
    "            importance_instance.append(maxabs_scale(fi(instance, neural_type)))\n",
    "        importance_test.append(importance_instance)\n",
    "    importance_test = np.array(importance_test)\n",
    "\n",
    "    importances = {'train': importance_train.tolist(),\n",
    "                   'test': importance_test.tolist()}\n",
    "    with open('D1_'+neural_name+'.txt', 'w') as outfile:\n",
    "        json.dump(importances, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our quantitative experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_functions = {'lNN': predict_lNN, 'NN': predict_NN, 'cNN': predict_cNN}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_test = np.array(importances['test'])\n",
    "    meta_explain = MetaExplain(importance_train, flat_features)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['weak', 'normal', 'strong']:\n",
    "        for delta in [0, 0.0001, 0.001, 0.01]:\n",
    "            my_altruist = Altruist(neural_type, x_train, fi_techniques,\n",
    "                                   flat_features, level=noise, delta=delta)\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(x_test[:1000])):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                temp_meta = []\n",
    "                temp_meta.append(meta_explain.meta_avg(b))\n",
    "                temp_meta.append(meta_explain.meta_median(b))\n",
    "                temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "                for i in range(len(temp_meta)):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in temp_meta[i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i+len(importance_test[j])].append(cnzw)\n",
    "                    nzw_scores_delta[i+len(importance_test[j])].append(cnzwt)\n",
    "                my_altruist.fis = len(meta_names)\n",
    "                a = my_altruist.find_untruthful(x_test[j], temp_meta)\n",
    "                for i in range(len(a[0])):\n",
    "                    meta_scores[i].append(len(a[0][i]))\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            for meta_score in meta_scores:\n",
    "                row.append(np.array(meta_score).mean())\n",
    "            with open('D1_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names+meta_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D1_NZW_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the interpretations on the Sensor level as well. This is  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_functions = {'lNN': predict_lNN, 'NN': predict_NN, 'cNN': predict_cNN}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'lNN':\n",
    "        fi_techniques = [fi_lNN, fi_IG_lNN, fi_LRP_lNN, fi_lime, fi_random]\n",
    "        fi_names = ['Inherent', 'IG', 'LRP', 'LIME', 'RAND']\n",
    "    elif neural_name == 'NN':\n",
    "        fi_techniques = [fi_IG_NN, fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_IG_cNN, fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_train_temp = []\n",
    "    importance_test = np.array(importances['test'])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_train:\n",
    "        importance_train_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    for i in importance_test:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_train = np.array(importance_train_temp)\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "\n",
    "    sensor_names = [str('F_'+str(i)) for i in range(14)]\n",
    "\n",
    "    meta_explain = MetaExplain(importance_train, sensor_names)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['weak', 'normal', 'strong']:\n",
    "        for delta in [0, 0.0001, 0.001, 0.01, 0.1]:\n",
    "            my_altruist = Altruist(neural_type, x_train, fi_techniques, sensor_names,\n",
    "                                   level=noise, delta=delta, data_type='per Sensor')\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(x_test[:1000])):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                temp_meta = []\n",
    "                temp_meta.append(meta_explain.meta_avg(b))\n",
    "                temp_meta.append(meta_explain.meta_median(b))\n",
    "                temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "                for i in range(len(temp_meta)):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in temp_meta[i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i+len(importance_test[j])].append(cnzw)\n",
    "                    nzw_scores_delta[i+len(importance_test[j])].append(cnzwt)\n",
    "                my_altruist.fis = len(meta_names)\n",
    "                a = my_altruist.find_untruthful(x_test[j], temp_meta)\n",
    "                for i in range(len(a[0])):\n",
    "                    meta_scores[i].append(len(a[0][i]))\n",
    "\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            for meta_score in meta_scores:\n",
    "                row.append(np.array(meta_score).mean())\n",
    "            with open('D1_PS'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names+meta_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D1_PS_NZW_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will perform the ablation study in per sensor level!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_functions = {'NN': predict_NN, 'cNN': predict_cNN}\n",
    "for neural_name, neural_type in predict_functions.items():\n",
    "    if neural_name == 'NN':\n",
    "        fi_techniques = [fi_LRP_NN, fi_lime, fi_random]\n",
    "        fi_names = ['LRP', 'LIME', 'RAND']  # ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    else:\n",
    "        fi_techniques = [fi_LRP_cNN, fi_lime, fi_random]\n",
    "        fi_names = ['LRP', 'LIME', 'RAND']  # ['IG', 'LRP', 'LIME', 'RAND']\n",
    "    meta_names = ['Average', 'Median', 'RuleBased']\n",
    "\n",
    "    with open(weights_path+'D1_'+neural_name+'.txt') as json_file:\n",
    "        importances = json.load(json_file)\n",
    "    importance_train = np.array(importances['train'])\n",
    "    importance_train_temp = []\n",
    "    importance_test = np.array(importances['test'])\n",
    "    importance_test_temp = []\n",
    "    for i in importance_train:\n",
    "        importance_train_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    for i in importance_test:\n",
    "        importance_test_temp.append(i.reshape((len(i), 50, 14)).mean(axis=1))\n",
    "    importance_train = np.array(importance_train_temp)\n",
    "    importance_test = np.array(importance_test_temp)\n",
    "    importance_train = np.delete(importance_train, 2, axis=1)\n",
    "    importance_test = np.delete(importance_test, 2, axis=1)\n",
    "\n",
    "    sensor_names = [str('F_'+str(i)) for i in range(14)]\n",
    "\n",
    "    meta_explain = MetaExplain(importance_train, sensor_names)\n",
    "\n",
    "    print('Starting evaluation for', neural_name)\n",
    "    for noise in ['normal']:\n",
    "        for delta in [0.0001]:\n",
    "            my_altruist = Altruist(neural_type, x_train, fi_techniques, sensor_names,\n",
    "                                   level=noise, delta=delta, data_type='per Sensor')\n",
    "            fis_scores = []\n",
    "            meta_scores = []\n",
    "            nzw_scores = []\n",
    "            nzw_scores_delta = []\n",
    "            for i in fi_techniques:\n",
    "                fis_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for i in meta_names:\n",
    "                meta_scores.append([])\n",
    "                nzw_scores.append([])\n",
    "                nzw_scores_delta.append([])\n",
    "            for j in range(len(x_test[:1000])):\n",
    "                my_altruist.fis = len(fi_techniques)\n",
    "                a = my_altruist.find_untruthful(x_test[j], importance_test[j])\n",
    "                for i in range(len(importance_test[j])):\n",
    "\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in importance_test[j][i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i].append(cnzw)\n",
    "                    nzw_scores_delta[i].append(cnzwt)\n",
    "                b = np.array(a[-1])\n",
    "                for i in range(len(a[0])):\n",
    "                    fis_scores[i].append(len(a[0][i]))\n",
    "                temp_meta = []\n",
    "                temp_meta.append(meta_explain.meta_avg(b))\n",
    "                temp_meta.append(meta_explain.meta_median(b))\n",
    "                temp_meta.append(meta_explain.meta_rule_based(a[0], a[2], b))\n",
    "                for i in range(len(temp_meta)):\n",
    "                    cnzw = 0\n",
    "                    cnzwt = 0\n",
    "                    for k in temp_meta[i]:\n",
    "                        if abs(k) > 0:\n",
    "                            cnzw += 1\n",
    "                        if abs(k) > delta:\n",
    "                            cnzwt += 1\n",
    "                    nzw_scores[i+len(importance_test[j])].append(cnzw)\n",
    "                    nzw_scores_delta[i+len(importance_test[j])].append(cnzwt)\n",
    "                my_altruist.fis = len(meta_names)\n",
    "                a = my_altruist.find_untruthful(x_test[j], temp_meta)\n",
    "                for i in range(len(a[0])):\n",
    "                    meta_scores[i].append(len(a[0][i]))\n",
    "\n",
    "            row = [neural_name, noise, delta]\n",
    "            for fis_score in fis_scores:\n",
    "                row.append(np.array(fis_score).mean())\n",
    "            for meta_score in meta_scores:\n",
    "                row.append(np.array(meta_score).mean())\n",
    "            with open('D1_PS_AblationIG'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "            row = [neural_name, noise, delta]\n",
    "            all_names = fi_names+meta_names\n",
    "            for aname in range(len(all_names)):\n",
    "                row.append(np.array(nzw_scores[aname]).mean())\n",
    "                row.append(np.array(nzw_scores_delta[aname]).mean())\n",
    "            with open('D1_PS_AblationIG_NZW_'+neural_name+'.csv', 'a', encoding='UTF8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f35bf8cd5f04ba351bb361a1212eeb730a988adc473d8146210fc49c7facefd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
